<!DOCTYPE html>
<html >
<head><meta charset="utf-8">
<title>MeDM: Mediating Image Diffusion Models for Video-to-Video Translation with Temporal Correspondence Guidance</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="google-site-verification" content="I9VHWgYS9g3o8RUJ7ryB1hPoIcN9YK1bX1Ds7_uDIYE">
<meta property="og:title" content="MeDM: Mediating Image Diffusion Models for Video-to-Video Translation with Temporal Correspondence Guidance">
<meta name="description" content="MeDM utilizes pre-trained image Diffusion Models for video-to-video translation with consistent temporal flow. The proposed framework can render videos from scene position information, such as a normal G-buffer, or perform text-guided editing on videos captured in real-world scenarios. We employ explicit optical flows to construct a practical coding that enforces physical constraints on generated frames and mediates independent frame-wise scores. By leveraging this coding, maintaining temporal consistency in the generated videos can be framed as an optimization problem with a closed-form solution. To ensure compatibility with Stable Diffusion, we also suggest a workaround for modifying observed-space scores in latent-space Diffusion Models. Notably, MeDM does not require fine-tuning or test-time optimization of the Diffusion Models.">
<meta property="og:description" content="MeDM utilizes pre-trained image Diffusion Models for video-to-video translation with consistent temporal flow. The proposed framework can render videos from scene position information, such as a normal G-buffer, or perform text-guided editing on videos captured in real-world scenarios. We employ explicit optical flows to construct a practical coding that enforces physical constraints on generated frames and mediates independent frame-wise scores. By leveraging this coding, maintaining temporal consistency in the generated videos can be framed as an optimization problem with a closed-form solution. To ensure compatibility with Stable Diffusion, we also suggest a workaround for modifying observed-space scores in latent-space Diffusion Models. Notably, MeDM does not require fine-tuning or test-time optimization of the Diffusion Models.">
<meta property="og:image" content="https://medm2023.github.io/images/system-diagram.jpg">
<meta name="twitter:card" content="summary_large_image"><link rel="preload" as="fetch" crossorigin="anonymous" href="/_payload.json"><link rel="modulepreload" as="script" crossorigin href="/assets/entry.66cb620f.js"><link rel="preload" as="style" href="/assets/entry.37dff817.css"><link rel="prefetch" as="script" crossorigin href="/assets/error-404.22a0bf03.js"><link rel="prefetch" as="script" crossorigin href="/assets/error-500.7c6f0f22.js"><link rel="stylesheet" href="/assets/entry.37dff817.css"><style>.header[data-v-e4fb420e]{margin:0 auto;max-width:1200px;padding:2rem}.title[data-v-e4fb420e]{font-size:2.1rem;margin-bottom:1rem}.title[data-v-e4fb420e],.venue[data-v-e4fb420e]{font-weight:500;text-align:center}.venue[data-v-e4fb420e]{font-size:1.5rem}.affiliation-list[data-v-e4fb420e],.author-list[data-v-e4fb420e]{-moz-column-gap:1.5rem;column-gap:1.5rem;display:flex;flex-wrap:wrap;font-size:1.2rem;justify-content:center;text-align:center}.author-list>div[data-v-e4fb420e]{white-space:nowrap}@media (max-width:850px){.title[data-v-e4fb420e] div{color:var(--color-text-mute);font-size:1.2rem}}@media (min-width:850px){.title[data-v-e4fb420e] div{display:inline;font-weight:inherit}.title[data-v-e4fb420e] div:before{content:": ";font-weight:inherit}}.link-list[data-v-e4fb420e]{display:flex;flex-wrap:wrap;font-size:1.2rem;gap:1rem 2rem;justify-content:center;margin-top:2rem}.link-list>a[data-v-e4fb420e]:after{all:initial}.link-list>a>div[data-v-e4fb420e]{background-color:var(--color-background-accent);border-radius:100px;color:var(--color-text-accent);padding:.2rem 1.3rem;transition:transform .2s}@media (hover:hover){.link-list>a>div[data-v-e4fb420e]:hover{transform:scale(1.05)}}.link-list>a>div[data-v-e4fb420e]:active{transform:scale(1.05)}@media (prefers-color-scheme:dark){.link-list>a>div[data-v-e4fb420e]{border:1px solid #fff}}i[data-v-e4fb420e]{margin-right:.1rem}</style><style>.medium-zoom-image--opened[data-v-194bb4af],.medium-zoom-overlay[data-v-194bb4af]{border:initial;border-radius:0;z-index:999}img[data-v-194bb4af]{width:100%}</style><style>.main[data-v-8d4a1237]{margin:0 auto;max-width:1200px;padding:2rem}.section-title[data-v-8d4a1237]{font-size:2rem;font-weight:500;margin:2rem 0 .5rem;text-align:center}.caption[data-v-8d4a1237]{font-size:1rem;line-height:1.6;margin-top:.2rem;padding:0 2.5rem;text-align:center}.image[data-v-8d4a1237]{margin:1.5rem 0 1rem}@media (max-width:650px){.caption[data-v-8d4a1237]{font-size:1rem;padding:0}}p[data-v-8d4a1237]{margin:.5rem 0 1rem}.bold[data-v-8d4a1237]{font-weight:700}</style><style>.footer[data-v-52b75ba3]:before{background-image:url(/images/backgrounds/wave.png);background-position:bottom;content:" ";height:188px;pointer-events:none;position:absolute;top:-187px;width:100%}.footer[data-v-52b75ba3],.footer[data-v-52b75ba3]:before{background-repeat:no-repeat;background-size:cover}.footer[data-v-52b75ba3]{background-color:var(--color-background-accent);background-image:url(/images/backgrounds/footer.jpg);background-position:top;margin-top:100px;max-width:100%;min-height:300px;position:relative;width:calc(100vw - var(--scrollbarWidth))}.footer[data-v-52b75ba3],.footer a[data-v-52b75ba3]{color:var(--color-text-accent)}.content[data-v-52b75ba3]{margin:0 auto;max-width:900px;padding:2rem;position:relative;z-index:1}.logo[data-v-52b75ba3]{display:flex;padding-bottom:calc(var(--section-gap)/4);place-items:flex-start}img[data-v-52b75ba3]{width:200px}@media (min-width:900px){.footer[data-v-52b75ba3]{display:flex;place-items:center}.content[data-v-52b75ba3]{display:grid;grid-template-columns:1fr 3fr;padding:0 2rem}.logo[data-v-52b75ba3]{padding-bottom:0;padding-right:calc(var(--section-gap)/2)}}</style><style>@import url("https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css");@import url("https://site-assets.fontawesome.com/releases/v6.4.2/css/all.css");:root{--vt-c-white:#fff;--vt-c-white-soft:#f8f8f8;--vt-c-white-mute:#f2f2f2;--vt-c-black:#181818;--vt-c-black-soft:#222;--vt-c-black-mute:#282828;--vt-c-indigo:#121314;--vt-c-divider-light-1:rgba(60,60,60,.29);--vt-c-divider-light-2:rgba(60,60,60,.12);--vt-c-divider-dark-1:rgba(84,84,84,.65);--vt-c-divider-dark-2:rgba(84,84,84,.48);--vt-c-text-light-1:var(--vt-c-indigo);--vt-c-text-light-2:rgba(0,0,0,.66);--vt-c-text-dark-1:var(--vt-c-white);--vt-c-text-dark-2:hsla(0,0%,92%,.64);--vt-c-asblue:#153146;--color-background:var(--vt-c-white);--color-background-soft:var(--vt-c-white-soft);--color-background-mute:var(--vt-c-white-mute);--color-background-accent:var(--vt-c-asblue);--color-border:var(--vt-c-divider-light-2);--color-border-hover:var(--vt-c-divider-light-1);--color-heading:var(--vt-c-text-light-1);--color-text:var(--vt-c-text-light-1);--color-text-mute:var(--vt-c-text-light-2);--color-text-accent:var(--vt-c-text-dark-1);--section-gap:160px}@media (prefers-color-scheme:dark){:root{--color-background:var(--vt-c-black);--color-background-soft:var(--vt-c-black-soft);--color-background-mute:var(--vt-c-black-mute);--color-background-accent:var(--vt-c-asblue);--color-border:var(--vt-c-divider-dark-2);--color-border-hover:var(--vt-c-divider-dark-1);--color-heading:var(--vt-c-text-dark-1);--color-text:var(--vt-c-text-dark-1);--color-text-mute:var(--vt-c-text-dark-2);--color-text-accent:var(--vt-c-text-dark-1)}}*,:after,:before{box-sizing:border-box;font-weight:400;margin:0}body{-webkit-font-smoothing:antialiased;-webkit-text-size-adjust:100%;-moz-osx-font-smoothing:grayscale;background:var(--color-background);color:var(--color-text);font-family:Inter,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Fira Sans,Droid Sans,Helvetica Neue,sans-serif;font-size:1em;line-height:1.6;text-rendering:optimizeLegibility;transition:color .5s,background-color .5s}:root{font-size:20px}@media (max-width:850px){:root{font-size:16px}}@media (max-width:450px){:root{font-size:12px}}#app{font-weight:400;margin:0 auto;max-width:1280px;padding:2rem}a{color:var(--color-text);cursor:pointer;position:relative;text-decoration:none;white-space:nowrap}a:after{background-color:rgba(179,153,59,.5);bottom:0;content:"";left:-.1em;position:absolute;right:-.1em;top:66%;transition:top .2s cubic-bezier(0,.8,.13,1);z-index:-1}@media (hover:hover){a:hover:after{top:0}}a:active:after{top:0}@media (min-width:1024px){body{display:flex;place-items:center}#app{display:grid;grid-template-columns:1fr 1fr;padding:0 2rem}}</style></head>
<body ><div id="__nuxt"><!--[--><div class="header" data-v-e4fb420e><div class="title" data-v-e4fb420e>MeDM<div>Mediating Image Diffusion Models for Video-to-Video Translation with Temporal Correspondence Guidance</div></div><div class="venue" data-v-e4fb420e>AAAI 2024</div><div class="author-list" data-v-e4fb420e><!--[--><div data-v-e4fb420e><a href="https://jwliao1209.github.io/" target="_blank" data-v-e4fb420e>Jia-Wei Liao</a><!----></div><div data-v-e4fb420e><!--[-->Winston Wang<!--]--><!----></div><div data-v-e4fb420e><!--[-->Tzu-Sian Wang<!--]--><!----></div><div data-v-e4fb420e><!--[-->Li-Xuan Peng<!--]--><!----></div><div data-v-e4fb420e><!--[-->Cheng-Fu Chou<!--]--><!----></div><div data-v-e4fb420e><a href="https://www.citi.sinica.edu.tw/pages/pullpull/" target="_blank" data-v-e4fb420e>Jun-Cheng Chen</a><!----></div><!--]--></div><div class="affiliation-list" data-v-e4fb420e><!--[--><div data-v-e4fb420e><!---->Research Center for Information Technology Innovation, Academia Sinica National Taiwan University</div><!--]--></div><div class="link-list" data-v-e4fb420e><!--[--><a href="https://arxiv.org/abs/2403.15878" target="_blank" data-v-e4fb420e><div data-v-e4fb420e><i class="ai ai-arxiv" data-v-e4fb420e></i> arXiv</div></a><a href="https://github.com/jwliao1209/DiffQRCode" target="_blank" data-v-e4fb420e><div data-v-e4fb420e><i class="fa-brands fa-github" data-v-e4fb420e></i> Code</div></a><a href="https://github.com/jwliao1209/DiffQRCode" target="_blank" data-v-e4fb420e><div data-v-e4fb420e><i class="fa-brands fa-github" data-v-e4fb420e></i> Citation</div></a><!--]--></div></div><div class="main" data-v-8d4a1237><div class="image" data-v-8d4a1237><img src="images/teaser.png" data-v-8d4a1237 data-v-194bb4af><div class="caption" data-v-8d4a1237>Leveraging the preeminent capability of Latent Diffusion Model (LDM) and ControlNet as a prior knowledge of aesthetic QR code images, coupled with our proposed Scanning-Robust (Perceptual) Guidance, we can generate custom-styled QR codes conform to user prompts while assuring both scannability and aesthetics.</div></div><div class="section-title" data-v-8d4a1237>Abstract</div><p data-v-8d4a1237> QR codes, prevalent in daily applications, lack visual appeal due to their conventional black-and-white design. Integrating aesthetics while maintaining scannability poses a challenge. In this paper, we introduce a novel diffusion-model-based aesthetic QR code generation pipeline, utilizing pre-trained ControlNet and guided iterative refinement via a novel classifier guidance (SRG) based on the proposed Scanning-Robust Loss (SRL) tailored with QR code mechanisms, which ensures both aesthetics and scannability. To further improve the scannability while preserving aesthetics, we propose a two-stage pipeline with Scanning-Robust Perceptual Guidance (SRPG). Moreover, we can further enhance the scannability of the generated QR code by postprocessing it through the proposed Scanning-Robust Projected Gradient Descent (SRPGD) post-processing technique based on SRL with proven convergence. With extensive quantitative, qualitative, and subjective experiments, the results demonstrate that the proposed approach can generate diverse aesthetic QR codes with flexibility in detail. In addition, our pipelines outperforming existing models in terms of Scanning Success Rate (SSR) 86.67% (+40%) with comparable aesthetic scores. The pipeline combined with SRPGD further achieves 96.67% (+50%). </p><div class="section-title" data-v-8d4a1237>Methodology</div><div class="image" data-v-8d4a1237><img src="images/loss.png" data-v-8d4a1237 data-v-194bb4af><div class="caption" data-v-8d4a1237><span class="bold" data-v-8d4a1237>Scanning-Robust Loss (SRL).</span> We emulate the scanning process using module pixel extraction and binarization to calculate the pixel-wise error matrix and module-wise optimization decision mask. Then we apply a Gaussian kernel to re-weight the error matrix. Finally, we mask the error matrix with the decision mask via Hadamard product, then take the average to form our SRL.</div></div><div class="image" data-v-8d4a1237><img src="images/one_stage_generation_pipeline.png" data-v-8d4a1237 data-v-194bb4af><div class="caption" data-v-8d4a1237><span class="bold" data-v-8d4a1237>Iterative refinement with Scanning Robustness Guidance (SGD).</span> First, we leverage pre-trained ControlNet to obtain the initial score prediction conditioned on the target QR code and user-specified prompt. During each denoising step, we approximate the original latent followed by DDIM formulation, then apply the VAE decoder to get the original image for SRL calculation. We utilize the gradient of SRL as a guidance term to update the predicted score. Repeat the above iterative refinement process until convergence.</div></div><div class="image" data-v-8d4a1237><img src="images/two_stage_generation_pipeline.png" data-v-8d4a1237 data-v-194bb4af><div class="caption" data-v-8d4a1237><span class="bold" data-v-8d4a1237>Two-stage generation pipeline with Scanning-Robust Perceptual Guidance (SRPG).</span> In Stage 1, we utilize the pre-trained plain ControlNet to generate an aesthetic yet unscannable sub-optimal QR code; In Stage 2, we first perform SDEdit to convert the sub-optimal QR code to latent space, then leverage Qart to merge with the target QR code, finally, we apply our proposed iterative refinement to produce aesthetic and scannable QR code.</div></div><div class="section-title" data-v-8d4a1237>Comparison Results</div><div class="image" data-v-8d4a1237><img src="images/comparison.png" data-v-8d4a1237 data-v-194bb4af><div class="caption" data-v-8d4a1237>Comparisons with generative-based methods. The green box represents scannable images, while the red box indicates images that cannot be scanned.</div></div><div class="image" data-v-8d4a1237><img src="images/compare.png" data-v-8d4a1237 data-v-194bb4af><div class="caption" data-v-8d4a1237>Quantitative results of generative-based methods and our proposed pipeline. Improvements marked in green are compared with QR Code Monster.</div></div><div class="section-title" data-v-8d4a1237>Analytics</div><p data-v-8d4a1237> In Fig. (a), we compare the error rates of a sample with different SRG weights during iterative refinement steps. We observed that the error plunges within the first 5 iterations with SRG, whereas without SRG. Furthermore, we analyze the change in score magnitude of different SRG weights. We found that the score magnitude decreased over the iterations, indicating the guidance effects diminished over time. This trend is depicted in Fig. (b). </p><div style="display:flex;" data-v-8d4a1237><div class="image" data-v-8d4a1237><img src="images/error.png" data-v-8d4a1237 data-v-194bb4af><div class="caption" data-v-8d4a1237>(a) QR code error rate.</div></div><div class="image" data-v-8d4a1237><img src="images/gradient_norm.png" data-v-8d4a1237 data-v-194bb4af><div class="caption" data-v-8d4a1237>(b) Score magnitude</div></div></div><p data-v-8d4a1237> We visualize the images at different timestep and their corresponding mismatched modules. The mismatched modules are marked in red, indicating the inconsistencies between scanner-decoded image and the target QR code, Initially, the image contains a plethora of mismatched modules, leading to the unscannable situation. However, the number of mismatched modules significantly decreases as the sampling process proceeds. Moreover, we can observe that the amount of mismatched modules plunges after certain sampling steps. This indicates that the mismatch rate falls within the QR code error-correction capacity, allowing the control reverting to the diffusion model to generate more appealing results. </p><div class="image" data-v-8d4a1237><img src="images/iterative_refinement_process.png" data-v-8d4a1237 data-v-194bb4af></div><p data-v-8d4a1237> We analyze the robustness of the generated results through error analysis. The scanning robustness can be maintained as long as the modules after sampling and binarization yield identical results as the target QR code regardless of pixel color changes within the modules. Our aesthetic QR codes exhibit irregular colors and shapes in their modules. Despite undergoing sampling and binarization, the module results remain consistent with the original QR code. This suggests that our aesthetic QR codes are robust and readable by a standard QR code scanner. </p><div class="image" data-v-8d4a1237><img src="images/error_analysis_qrcode_module_error.png" data-v-8d4a1237 data-v-194bb4af></div></div><div class="footer" data-v-52b75ba3><div class="content" data-v-52b75ba3><div class="logo" data-v-52b75ba3><img src="/images/logos/as-white.png" alt="" data-v-52b75ba3></div><div data-v-52b75ba3> This website is licensed under a Creative Commons Attribution-ShareAlike 4.0 International <a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" data-v-52b75ba3>License</a>. <br data-v-52b75ba3><br data-v-52b75ba3> This means you are free to borrow the <a href="https://github.com/medm2023/medm2023.github.io" target="_blank" data-v-52b75ba3>source code</a> of this website, we just ask that you link back to this page in the footer. However, the copyright to the logo of Academia Sinica and the footer theme is reserved. Please remove them if your are not affiliated with Academia Sinica. </div></div></div><!--]--></div><script type="application/json" id="__NUXT_DATA__" data-ssr="true" data-src="/_payload.json">[{"state":1,"_errors":3,"serverRendered":5,"path":6,"prerenderedAt":7},["Reactive",2],{},["Reactive",4],{},true,"/",1713989162645]</script><script>window.__NUXT__={};window.__NUXT__.config={public:{},app:{baseURL:"/",buildAssetsDir:"assets",cdnURL:""}}</script><script type="module" src="/assets/entry.66cb620f.js" crossorigin></script></body>
</html>